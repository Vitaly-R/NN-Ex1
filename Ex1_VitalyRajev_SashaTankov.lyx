#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1cm
\topmargin 2cm
\rightmargin 1cm
\bottommargin 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard

\series bold
\bar under
Introduction to Neural Networks - Ex1
\end_layout

\begin_layout Standard
Vitaly Rajev - 320929227
\end_layout

\begin_layout Standard
Sasha Tankov - 327337903
\end_layout

\begin_layout Section
Practical Questions
\end_layout

\begin_layout Paragraph
1.
 Train and Test 
\begin_inset Newline newline
\end_inset

1.1 - Number of free parameters of the network.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Input layer: 
\begin_inset Formula $(batch,28,28,1)\rightarrow(batch,28,28,1)$
\end_inset

, Convolution layer 1: 
\begin_inset Formula $(batch,28,28,1)\rightarrow(batch,24,24,32)$
\end_inset

 , Max pool layer 1: 
\begin_inset Formula $(batch,24,24,32)\rightarrow(batch,12,12,32)$
\end_inset

, Convolution layer 2: 
\begin_inset Formula $(batch,12,12,32)\rightarrow(batch,8,8,64)$
\end_inset

, Max pool layer 2: 
\begin_inset Formula $(batch,8,8,64)\rightarrow(batch,4,4,64)$
\end_inset

, Flattening layer: 
\begin_inset Formula $(batch,4,4,64)\rightarrow(batch,1024)$
\end_inset

, Dense layer 1: 
\begin_inset Formula $(batch,1024)\rightarrow(batch,1024)$
\end_inset

, Dense layer 2: 
\begin_inset Formula $(batch,1024)\rightarrow(batch,10)$
\end_inset

.
\end_layout

\begin_layout Standard
Input layer parameters - 
\begin_inset Formula $0$
\end_inset


\end_layout

\begin_layout Standard
Convolution layer 1 parameters - There are 
\begin_inset Formula $24\cdot24$
\end_inset

 pixels in each image that are used as center for convolution, each filter
 is of size 
\begin_inset Formula $5\cdot5$
\end_inset

 and there are 
\begin_inset Formula $32$
\end_inset

 filters.
 Combined, there are 
\begin_inset Formula $24\cdot24\cdot5\cdot5\cdot32=460,000$
\end_inset

 parameters in the layer.
\end_layout

\begin_layout Standard
Max pool layer 1 parameters - 0
\end_layout

\begin_layout Standard
Convolution layer 2 parameters - There are 
\begin_inset Formula $8\cdot8$
\end_inset

 pixels in each image that are used as center for the convolution, each
 pixel has 
\begin_inset Formula $32$
\end_inset

 channels, and the window of each filter is 
\begin_inset Formula $5\cdot5$
\end_inset

, meaning the size of each filter is 
\begin_inset Formula $5\cdot5\cdot32$
\end_inset

, and there are 64 filters.
 Combined, there are 
\begin_inset Formula $8\cdot8\cdot5\cdot5\cdot32\cdot64=3,276,800$
\end_inset

 parameters in the layer.
\end_layout

\begin_layout Standard
Max pool layer 2 parameters - 0
\end_layout

\begin_layout Standard
Flattening layer parameters - 0
\end_layout

\begin_layout Standard
Dense layer 1 parameters - There are 
\begin_inset Formula $1024$
\end_inset

 neurons in the layer, each one with a bias and 
\begin_inset Formula $1024$
\end_inset

 inputs.
 Combined, there are 
\begin_inset Formula $(1024+1)\cdot1024=1,049,600$
\end_inset

 parameters.
\end_layout

\begin_layout Standard
Dense layer 2 parameters - There are 
\begin_inset Formula $10$
\end_inset

 neurons in the layer, each one with a bias and 
\begin_inset Formula $1024$
\end_inset

 inputs.
 Combined, there are 
\begin_inset Formula $(1024+1)\cdot10=10,250$
\end_inset

 parameters.
\end_layout

\begin_layout Standard
Total number of parameters - 
\begin_inset Formula $460,800+3,276,800+1,049,600+10,250=4,797,450$
\end_inset

 parameters in the network.
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Paragraph
1.2 - Training/Test accuracy plots:
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
(Final trainng accuracy - 99.4%)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 1.2 Original CNN Training Accuracy.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
(Final test accuracy - 96.91%)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 1.2 Original CNN Test Accuracy.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Paragraph
2.
 Linear vs.
 Non-Linear
\end_layout

\begin_layout Paragraph
2.1 - Identifying non linear components and removing them
\end_layout

\begin_layout Standard
- The non-linear parts of the network are the ReLU activation functions
 after each convolution, the Maxpool operations, and the final softmax activatio
n.
 The ReLU activations and Maxpool layers were removed.
\end_layout

\begin_layout Standard
This was done by passing None as the activation of the hidden layers, and
 by setting the Maxpool layers to be None when getting 1 as a parameter
 (thus, cancelling the layer).
\end_layout

\begin_layout Paragraph
2.2 - Comparison of Results
\end_layout

\begin_layout Standard
The resulting accuracy measures:
\end_layout

\begin_layout Standard
(Final training accuracy - 90.13%)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 2.2 Linear Version of Original CNN Training Accuracy.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
(Final test accuracy - 87.49%)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 2.2 Linear Version of Original CNN Test Accuracy.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Paragraph
2.3 - Effect on Training and Final Test Accuracy
\end_layout

\begin_layout Standard
The effect on the training and testing of the network is that after removing
 the non-linear components, the training and test accuracies dropped.
 This is due to the network being strictly linear (except for the softmax
 activation), and the digit recognition task not being linear, thus limiting
 the networks ability to learn as effectively.
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Paragraph
3.
 Deep vs, Shallow
\end_layout

\begin_layout Paragraph
3.1 - Constructing a Shallow NN 
\end_layout

\begin_layout Standard
3.1.1 - Number of Parameters:
\end_layout

\begin_layout Standard
In the convolution layer, each filter is with window size of 
\begin_inset Formula $3\cdot3$
\end_inset

 and there are 
\begin_inset Formula $3$
\end_inset

 filters.
 There are 26 pixels in the input image for which the convolution will be
 calculated, each with 1 channel.
 In total: 
\begin_inset Formula $3\cdot3\cdot3\cdot26\cdot26\cdot1=18,252$
\end_inset

 parameters.
\end_layout

\begin_layout Standard
In the flattening layer there are 0 parameters, and the resulting vector
 is of length 2,028
\end_layout

\begin_layout Standard
In the dense layer there are 
\begin_inset Formula $10$
\end_inset

 neurons, each one with 
\begin_inset Formula $2,028$
\end_inset

 inputs and a bias.
 In total: 
\begin_inset Formula $(2,028+1)\cdot10=20,290$
\end_inset

 parameters.
\end_layout

\begin_layout Standard
In total, there are 
\begin_inset Formula $18,252+20,290=38,542$
\end_inset

 parameters in the network.
\end_layout

\begin_layout Standard
3.1.2 - Loss function of the training of the network:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 3.1 Shallow CNN Training Loss.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Paragraph
3.2 - Reducing the Original Network
\end_layout

\begin_layout Standard
3.2.1 - Number of parameters:
\end_layout

\begin_layout Standard
In the first convolution layer, there are 4 filters, each with window size
 of 
\begin_inset Formula $3\cdot3$
\end_inset

, and there are 
\begin_inset Formula $26\cdot26$
\end_inset

 pixels, each with 1 channel, over which the convolution will go.
 In total: 
\begin_inset Formula $4\cdot3\cdot3\cdot1\cdot26\cdot26=24,336$
\end_inset

 parameters.
 The shape of each resulting image is (26, 26, 4)
\end_layout

\begin_layout Standard
In the first maxpool layer there are no parameters.
 The shape of the resulting images is (13, 13, 4).
\end_layout

\begin_layout Standard
In the second convolution layer, there are 8 filters, each with a 
\begin_inset Formula $3\cdot3$
\end_inset

 window, there are 
\begin_inset Formula $11\cdot11$
\end_inset

 pixels over which the convolution is calculated, each pixel with 4 input
 channels.
 In total: 
\begin_inset Formula $11\cdot11\cdot4\cdot8\cdot3\cdot3=34,848$
\end_inset

 parameters.
\end_layout

\begin_layout Standard
In the second maxpool layer there are no parameters.
 The shape of the resulting images is (5, 5, 8).
\end_layout

\begin_layout Standard
In the flattening layer there are no parameters, and the resulting vector
 is of length 200.
\end_layout

\begin_layout Standard
In the first dense layer there are 10 neurons, each with a bias and 200
 inputs.
 In total: 
\begin_inset Formula $(200+1)\cdot10=2,010$
\end_inset

 parameters.
\end_layout

\begin_layout Standard
In the second dense layer there are 10 neurons, each with 2,010 inputs and
 a bias.
 In total: 
\begin_inset Formula $(2,010+1)\cdot10=20,110$
\end_inset

 parameters.
\end_layout

\begin_layout Standard
In total, there are 
\begin_inset Formula $24,336+34,848+2,010+20,110=81,304$
\end_inset

 parameters in the network.
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
3.2.2 - Training loss and accuracy of the network:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 3.2 Reduced Original Network Training Loss.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 3.2 Reduced Original Network Training Accuracy.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Paragraph
3.3 - Comparing Convergence
\end_layout

\begin_layout Standard
From the plots of the loss of each network, we can see that the shallow
 network converges faster (as there are less parameters to learn), and on
 the other hand, from printing the loss and accuracy values in each round,
 we got that the reduced original network converges to a lower training
 loss, and to a higher test accuracy.
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Paragraph
4.
 Overfitting
\end_layout

\begin_layout Standard
4.1 
\end_layout

\begin_layout Standard
Plots of the original networks (no dropout) training accuracy, test accuracy,
 and their ratio when training over all examples:
\end_layout

\begin_layout Standard
(Final tarining accuracy value - 99.4%)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 4.1 Original Network Without Dropout Training Accuracy Full Example Set.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
(Final test accuracy value - 96.77%)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 4.1 Original Network Without Dropout Test Accuracy Full Example Set.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
(Final ratio - 0.9735)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 4.1 Original Network Without Dropout Test to Training Accuracy Ratio Full Example Set.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
Plots of the original networks (no dropout) training accuracy, test accuracy,
 and their ratio when training over a reduced set of 250 examples:
\end_layout

\begin_layout Standard
(Final training accuracy - 100%)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 4.1 Original Network Without Dropout Training Accuracy Reduced Example Set.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
(Final test accuracy - 99.05%)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 4.1 Original Network Without Dropout Test Accuracy Reduced Example Set.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
(Final ratio - 0.9905)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 4.1 Original Network Without Dropout Test to Training Accuracy Ratio Reduced Example Set.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
Plots of the reduced networks (with dropout) training accuracy, test accuracy,
 and their ratio when training over all examples in the dataset:
\end_layout

\begin_layout Standard
(Final training accuracy - 99.37%)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 4.1 Original Network With Dropout Training Accuracy Full Example Set.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
(Final test accuracy - 96.7%)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 4.1 Original Network With Dropout Test Accuracy Full Example Set.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
(Final ratio - 0.9731)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 4.1 Original Network With Dropout Test to Training Accuracy Ratio Full Example Set.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
Plots of the reduced networks (with dropout) training accuracy, test accuracy,
 and their ratio when training over a reduced set of 250 examples:
\end_layout

\begin_layout Standard
(Final training accuracy - 99.999%)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 4.1 Original Network With Dropout Training Accuracy Reduced Example Set.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
(Final test accuracy - 99.04%)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 4.1 Original Network With Dropout Test Accuracy Reduced Example Set.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
(Final ratio - 0.9904)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 4.1 Original Network With Dropout Test to Training Accuracy Ratio Reduced Example Set.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Paragraph
5.
 Two Digits Sum
\end_layout

\begin_layout Standard
5.1 - Designed network architecture: 
\end_layout

\begin_layout Standard
Convolution layer with 32 filters, each with a 
\begin_inset Formula $5\times5$
\end_inset

 window size, and a ReLU activation.
\end_layout

\begin_layout Standard
Maxpool layer of 
\begin_inset Formula $2\times2$
\end_inset

 window.
\end_layout

\begin_layout Standard
Convolution layer with 64 filters, each with a 
\begin_inset Formula $5\times5$
\end_inset

 window size, and a ReLU activation.
\end_layout

\begin_layout Standard
Maxpool layer of 
\begin_inset Formula $2\times2$
\end_inset

 window.
\end_layout

\begin_layout Standard
Flattening layer (result - vector of length 2,048).
\end_layout

\begin_layout Standard
Dense layer 1 - with 2,048 neurons, and a ReLU activation.
\end_layout

\begin_layout Standard
Dense layer 2 - with 20 neurons, and a ReLU activation.
\end_layout

\begin_layout Standard
Dense layer 3 - with 19 neurons and a Softmax activation - output.
\end_layout

\begin_layout Standard
5.1.1 - Plots of the training and test accuracies of the network:
\end_layout

\begin_layout Standard
(Final training accuracy - 95.466%)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 5.1 Digits Sum Prediction as Concatenated Input Training accuracy.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
(Final test accuracy - 92.648%)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 5.1 Digits Sum Prediction as Concatenated Input Test accuracy.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
5.2 - Designed network architecture:
\end_layout

\begin_layout Standard
Convolution layer with 32 filters, each with window size 
\begin_inset Formula $5\times5$
\end_inset

 and a ReLU activation, for each image.
\end_layout

\begin_layout Standard
Maxpool layer of 
\begin_inset Formula $2\times2$
\end_inset

 window, for each image.
\end_layout

\begin_layout Standard
Convolution layer with 64 filters, each with a 
\begin_inset Formula $5\times5$
\end_inset

 window size, and a ReLU activation, for each image.
\end_layout

\begin_layout Standard
Maxpool layer of 
\begin_inset Formula $2\times2$
\end_inset

 window, for each image.
\end_layout

\begin_layout Standard
Flattening layer for each image.
\end_layout

\begin_layout Standard
Dense layer with 1024 neurons and a ReLU activation for each image.
\end_layout

\begin_layout Standard
Dense layer with 1024 neurons and a ReLU activation, connected to both parts
 of the previous dense layer (meaning, each neuron had 2,048 inputs).
\end_layout

\begin_layout Standard
Dense layer with 19 neurons and a Softmax activation - output.
\end_layout

\begin_layout Standard
5.2.1 - Plots of the training and test accuracies of the network:
\end_layout

\begin_layout Standard
(Final training accuracy - 97.418%)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 5.2 Digits Sum Prediction as Separate Inputs Training accuracy.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
(Final test accuracy - 95.178%)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Graphs/Question 5.2 Digits Sum Prediction as Separate Inputs Test accuracy.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Standard
5.3 - Predicting each digit separately and then preforming the sum worked
 better, since the network first predicted each number (up to the first
 dense layer - since up to that point, it was identical to the original
 network from question 1), and after concatenating the results of the prediction
s, the second and third dense layers could calculate the sum.
 On the other hand, the network with the concatenated input got two digits
 in one image and was was trained to predict the sum, rather than to recognize
 the digits, since it processed both numbers as a single input, it does
 not distinguish between numbers.
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section
Theoretical Questions
\end_layout

\begin_layout Paragraph
1.
 Convolution
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
1.1.
\end_layout

\begin_layout Standard
By definition of convolution: 
\begin_inset Formula $(f(k)*g(k))(n)=\sum_{k=-\infty}^{\infty}f(n-k)g(k)$
\end_inset

, by shifting the input of 
\begin_inset Formula $f$
\end_inset

 by 
\begin_inset Formula $t$
\end_inset

, we get that 
\end_layout

\begin_layout Standard
\begin_inset Formula $(f(k+t)*g(k))(n)=\sum_{k=-\infty}^{\infty}f(t+(n-k))g(k)=_{(*)}\sum_{k=-\infty}^{\infty}f((n+t)-k)g(k)=_{(definition-convolution)}(f(k)*g(k))(n+t)$
\end_inset

.
\end_layout

\begin_layout Standard
By commutativity of convolution: 
\begin_inset Formula $\sum_{k=-\infty}^{\infty}f(n-k)g(k)=\sum_{k=-\infty}^{\infty}f(k)g(n-k)$
\end_inset

, thus, from equivalence 
\begin_inset Formula $(*)$
\end_inset

, we have: 
\begin_inset Formula $\sum_{k=-\infty}^{\infty}f(t+(n-k))g(k)=_{(*)}\sum_{k=-\infty}^{\infty}f((n+t)-k)g(k)=\sum_{k=-\infty}^{\infty}f(k)g((n+t)-k)=\sum_{k=-\infty}^{\infty}f(k)g(t+(n-k))=(f(k)*g(k+t))(n)$
\end_inset

.
\end_layout

\begin_layout Standard
Therefore, we conclude that 
\begin_inset Formula $(f(k+t)*g(k))(n)=(f(k)*g(k))(n+t)=(f(k)*g(k+t))(n)$
\end_inset

, and get that convolution is translation-invariant.
\end_layout

\begin_layout Standard
1.2.1
\end_layout

\begin_layout Standard
When convolving a signal of length 
\begin_inset Formula $n$
\end_inset

 with a filter of length 
\begin_inset Formula $k\le n$
\end_inset

, and droppint all cases where the filter is not fully included in the signal,
 the length of the output signal is the number of ways we can place the
 entire filter in the signal (without changing the filter in any way).
 If the leftmost component of the filter is at position 
\begin_inset Formula $t$
\end_inset

 in the input signal, then the position of the rightmost component is 
\begin_inset Formula $t+k-1$
\end_inset

.
 Since we require the filter to be fully included in the signal, and assuming
 the indices are 
\begin_inset Formula $1$
\end_inset

 to 
\begin_inset Formula $n$
\end_inset

, we get that: 
\begin_inset Formula $1\le t$
\end_inset

 and 
\begin_inset Formula $t+k-1\le n$
\end_inset

, therefore the minimal 
\begin_inset Formula $t$
\end_inset

 is 1 and the maximal 
\begin_inset Formula $t$
\end_inset

 is 
\begin_inset Formula $n-k+1$
\end_inset

.
 Since 
\begin_inset Formula $t$
\end_inset

 is discrete, we get that there are 
\begin_inset Formula $n-k+1$
\end_inset

 ways to position the filter in the input signal without changing it, and
 the length of the output signal is 
\begin_inset Formula $n-k+1$
\end_inset

.
\end_layout

\begin_layout Standard
1.2.2
\end_layout

\begin_layout Standard
As seen in the previous question - the kength of the output signal is 
\begin_inset Formula $n-k+1$
\end_inset

.
 Since 
\begin_inset Formula $n=k$
\end_inset

, we get that the length of the output signal is 
\begin_inset Formula $1$
\end_inset

, and the result is the inner product of the filter and the input signal.
\end_layout

\begin_layout Paragraph
2.
 ReLU
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
2.1 
\end_layout

\begin_layout Standard
Given the function 
\begin_inset Formula $F(relu(f*l+b))$
\end_inset

 , we can compute the partial derivative using the chain rule: 
\begin_inset Formula $\frac{\partial F}{\partial f}(n)=\frac{\partial F}{\partial relu}\frac{\partial relu}{\partial(f*l+b)}\frac{\partial(f*l+b)}{\partial f}(n)$
\end_inset

 .
 Since convolution is a linear operator, we have that 
\begin_inset Formula $\frac{\partial(f*l+b)}{\partial f}=l$
\end_inset

 ,we also have that 
\begin_inset Formula $\frac{\partial relu}{\partial(f*l+b)}=\begin{cases}
1 & f*l+b>0\\
0 & otherwise
\end{cases}$
\end_inset

.
\end_layout

\begin_layout Standard
So we get that 
\begin_inset Formula $\frac{\partial F}{\partial f}(n)=\begin{cases}
\frac{\partial F}{\partial relu}\cdot l & f*l+b>0\\
0 & f*l+b\le0
\end{cases}$
\end_inset

, and 
\begin_inset Formula $\frac{\partial F}{\partial b}(n)=\begin{cases}
\frac{\partial F}{\partial relu} & f*l+b>0\\
0 & f*l+b\le0
\end{cases}$
\end_inset

.
\end_layout

\begin_layout Standard
2.2
\end_layout

\begin_layout Standard
In our case 
\begin_inset Formula $f*l+b<0$
\end_inset

 for all n, so we get that 
\begin_inset Formula $\frac{\partial relu}{\partial(f*l+b)}=0\Rightarrow\frac{\partial F}{\partial f}=0$
\end_inset

 (from the chain rule).
 
\end_layout

\begin_layout Standard
So, the gradient descent algorithm will have no effect on this layer, and
 the filters and biases will not be updated.
\end_layout

\begin_layout Paragraph
3.
 Backpropagation
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
3.1
\end_layout

\begin_layout Standard
Convolutionisa linear operator, so if we have a filter 
\begin_inset Formula $f=(a_{ij})_{1\le j\le n,1\le i\le n}$
\end_inset

, and a signal 
\begin_inset Formula $l=(b_{1},...,b_{n})$
\end_inset

 , let us define 
\begin_inset Formula $g(l)=f*l$
\end_inset

.
 
\end_layout

\begin_layout Standard
It can be expressed in matrix form as 
\begin_inset Formula $m=(x_{1},...,x_{n})$
\end_inset

 where 
\begin_inset Formula $x_{i}$
\end_inset

 is a row vector in form of 
\end_layout

\begin_layout Standard
\begin_inset Formula $x_{i}=(0,0,...,a_{1,1},...,a_{1,n},0,..,0,a_{2,1},....,a_{2,n},0,...,0,...,a_{n,1},...,a_{n,n},0,..,0)$
\end_inset

 shifted 
\begin_inset Formula $i$
\end_inset

 placed to the right, so we can write 
\begin_inset Formula $g(l)=ml$
\end_inset

.
 
\end_layout

\begin_layout Standard
So if the operation is linear then the derivative w.r.t 
\begin_inset Formula $f$
\end_inset

 is a matrix where each row if a row vector 
\begin_inset Formula $l$
\end_inset

.
\end_layout

\begin_layout Standard
3.2
\end_layout

\begin_layout Standard
When multiplying by a gradient vector, we can simply compute the dot product
 of the first row of the matrix with the gradient vector, and propagate
 the result to every position in the result vector.
 For matrix 
\begin_inset Formula $n=(y_{1},...,y_{n})$
\end_inset

, where 
\begin_inset Formula $y_{i}$
\end_inset

 is a row vector and for every 
\begin_inset Formula $i$
\end_inset

 we have that 
\begin_inset Formula $y_{i}=l$
\end_inset

.
\end_layout

\begin_layout Standard
The product 
\begin_inset Formula $n\cdot\partial h$
\end_inset

 (
\begin_inset Formula $\partial h$
\end_inset

 is some gradient vector) is 
\begin_inset Formula $z=(z_{1},...,z_{n})$
\end_inset

 where 
\begin_inset Formula $z_{i}=\sum_{j}y_{1,j}\cdot\partial h_{j}$
\end_inset

, it is more efficient than a regular matrix-vector multiplication.
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Paragraph
4.
 Parameters vs.
 Constraints
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
4.1 
\end_layout

\begin_layout Standard
Let us denote the number of pixel in the image over which we calculate the
 convolution as 
\begin_inset Formula $h,w$
\end_inset

 (meaning, the result will have a shape 
\begin_inset Formula $(h,w)$
\end_inset

).
\end_layout

\begin_layout Standard
There are 3 input channels to each neuron, and the window size for each
 nuron is 
\begin_inset Formula $5\cdot5$
\end_inset

.
 Each neuron also has a bias.
 The number of parameters for each neuron is 
\begin_inset Formula $5\cdot5\cdot3\cdot h\cdot w+1$
\end_inset

.
 Since there are 96 output channels, the total number of parameters is 
\begin_inset Formula $(75\cdot h\cdot w+1)\cdot96$
\end_inset

.
\end_layout

\begin_layout Standard
If we were to use the mnist dataset and the original network from the practical
 part, we would have that 
\begin_inset Formula $h=w=24$
\end_inset

 and the total number of parameters would be 
\begin_inset Formula $(75\cdot24^{2}+1)\cdot96=4,196,352$
\end_inset

.
\end_layout

\begin_layout Standard
4.2
\end_layout

\begin_layout Standard
The number of inputs to each neuron is 
\begin_inset Formula $4\cdot4\cdot256=4,096$
\end_inset

.
 Since the layer maps the image to the same dimensions, there are 
\begin_inset Formula $4,096$
\end_inset

 neurons, each one having 
\begin_inset Formula $4,096$
\end_inset

 inputs and a bias.
 In total, the number of parameters is 
\begin_inset Formula $4,096\cdot(4,096+1)=16,781,312$
\end_inset

.
\end_layout

\begin_layout Standard
4.3
\end_layout

\begin_layout Standard
-
\end_layout

\end_body
\end_document
